{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started\n",
    "#### To start this notebook, you must have a huggingface account and request access from Meta to use Llama 2.\n",
    "https://huggingface.co/\n",
    "\n",
    "#### In huggingface, create an access token\n",
    "https://huggingface.co/docs/hub/security-tokens\n",
    "\n",
    "#### Inside your home directory create .apikeys and inside that directory create a huggingface_api_key.txt and paste you access token inside the file. The path should look like the following.\n",
    "path - /home/{your username}/.apikeys\n",
    "\n",
    "#### Using the link below request access from Meta\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "#### Once you recieve access from Meta inside terminal create a conda environment using\n",
    "conda create --name {environment_name} python=3.10\n",
    "\n",
    "#### Then Install ipykernel using\n",
    "conda install ipykernel\n",
    "\n",
    "#### To allow your environment to be used in the notebook run the following line and select your environment on the top right besides the debugging symbol\n",
    "python -m ipykernel install --user --name={environment_name}\n",
    "\n",
    "#### Go back to terminal and install all the packages with\n",
    "pip install -r packages.txt\n",
    "\n",
    "#### Edit the data set, test set, validation set, and prompt/prompt template function under Load Datasets with the path and you are good to go!\n",
    "##### All imported data must be a csv. Csv must have at least 2 columns for output and test\n",
    "##### Import the data inside the llama-recipes folder\n",
    "\n",
    "#### If you are just loading a model run the first 4 cells then skip to the loading section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Huggingface API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/kwamea/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory and load Huggingface api key\n",
    "import os\n",
    "\n",
    "username = os.getenv('USER')\n",
    "directory_path = os.path.join('/scratch', username)\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "\n",
    "# Set Huggingface cache directory to be on scratch drive\n",
    "if os.path.exists(directory_path):\n",
    "    hf_cache_dir = os.path.join(directory_path, 'hf_cache')\n",
    "    if not os.path.exists(hf_cache_dir):\n",
    "        os.mkdir(hf_cache_dir)\n",
    "    print(f\"Okay, using {hf_cache_dir} for huggingface cache. Models will be stored there.\")\n",
    "    assert os.path.exists(hf_cache_dir)\n",
    "    os.environ['TRANSFORMERS_CACHE'] = f'/scratch/{username}/hf_cache/'\n",
    "else:\n",
    "    error_message = f\"Are you sure you entered your username correctly? I couldn't find a directory {directory_path}.\"\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load Huggingface api key\n",
    "api_key_loc = os.path.join('/home', username, '.apikeys', 'huggingface_api_key.txt')\n",
    "\n",
    "if os.path.exists(api_key_loc):\n",
    "    print('Huggingface API key loaded.')\n",
    "    with open(api_key_loc, 'r') as api_key_file:\n",
    "        huggingface_api_key = api_key_file.read().strip()  # Read and store the contents\n",
    "else:\n",
    "    error_message = f'Huggingface API key not found. You need to get an HF API key from the HF website and store it at {api_key_loc}.\\n' \\\n",
    "                    'The API key will let you download models from Huggingface.'\n",
    "    raise FileNotFoundError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/kwamea/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, DataCollatorWithPadding, default_data_collator, Trainer, TrainingArguments\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from configs import fsdp_config, train_config\n",
    "from peft import get_peft_model, prepare_model_for_int8_training, PeftModelForCausalLM, LoraConfig, TaskType, prepare_model_for_int8_training, PeftModel\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "from utils.train_utils import (\n",
    "    train,\n",
    "    freeze_transformer_layers,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    clear_gpu_cache,\n",
    "    print_model_size,\n",
    ")\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "from configs.datasets import samsum_dataset, alpaca_dataset, grammar_dataset\n",
    "from ft_datasets.utils import Concatenator\n",
    "import huggingface_hub\n",
    "from huggingface_hub import notebook_login, Repository, HfApi, create_repo, delete_repo\n",
    "huggingface_hub.login(token = huggingface_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-v0.1\", #mistralai/Mistral-7B-v0.1\n",
    "        resume_download=True,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        token=huggingface_api_key,\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "#the code will output \"Error displaying widget: model not found\" it is not an error just the code failing to create a loading bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ad50395cf24c7889144be6040e204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-v0.1\", #meta-llama/Llama-2-7b-hf\n",
    "        resume_download=True,\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        trust_remote_code = True,\n",
    "        token=huggingface_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-04ed86c0369d6106/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-98e7740e620c6c6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-ed9c43e8e741b026/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-04ed86c0369d6106/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-19ca57271f564c64.arrow\n",
      "Loading cached processed dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-98e7740e620c6c6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-538cc2747fc973be.arrow\n",
      "Loading cached processed dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-ed9c43e8e741b026/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-07e4c8e46808a70f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efce0cbb07014d918d080d2492873151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f247ec8d5c44648994994f642f9c197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95820f976fbb48298f4c136d0371f683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e945a62e509407da800278fbcf8541a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3f01f9c65b406aa002bca3a1a030f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb88bd74ca34a46ad2978b810fd8304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add testset and rename current test set to validation set \n",
    "\n",
    "#edit file path to your unique dataset\n",
    "dataset = load_dataset('csv', data_files='samsum-data/samsum-train.csv',split = 'train')\n",
    "valset = load_dataset('csv', data_files='samsum-data/samsum-validation.csv',split = 'train')\n",
    "testset = load_dataset('csv', data_files='samsum-data/samsum-test.csv',split = 'train')\n",
    "\n",
    "shorten_percentage = 10\n",
    "\n",
    "# Shorten the datasets\n",
    "def shorten_dataset(dataset, percentage):\n",
    "    num_samples = len(dataset)\n",
    "    new_num_samples = int(num_samples * (percentage / 100.0))\n",
    "    return dataset.shuffle(seed=42).select(list(range(new_num_samples)))\n",
    "\n",
    "# Shorten each dataset\n",
    "\"\"\"\n",
    "dataset = shorten_dataset(dataset, shorten_percentage)\n",
    "valset = shorten_dataset(valset, shorten_percentage)\n",
    "testset = shorten_dataset(testset, shorten_percentage)\n",
    "\"\"\"\n",
    "#Edit the prompt to tell the model what to do including the variables from prompt.format\n",
    "prompt = (\n",
    "    f\"Summarize this dialogue:\\n{{dialog}}\\n---\\nSummary:{{summary}}\\n\"\n",
    ")\n",
    "\n",
    "#prompt for testing\n",
    "test_prompt = (\n",
    "    f\"Summarize this dialogue:\\n{{dialog}}\\n---\\nSummary:\\n\"\n",
    ")\n",
    "\n",
    "#edit the variables in prompt.format to match your data: essentially what you what the model to read\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            dialog = sample[\"dialogue\"],\n",
    "            summary = sample[\"summary\"],\n",
    "        )\n",
    "    }\n",
    "\n",
    "#Only include what you want the model to see during testing\n",
    "def apply_prompt_template_TEST(sample):\n",
    "    return {\n",
    "        \"text\": test_prompt.format(\n",
    "            dialog = sample[\"dialogue\"],\n",
    "        )\n",
    "    }\n",
    "\n",
    "data = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "val = valset.map(apply_prompt_template, remove_columns=list(valset.features))\n",
    "test = testset.map(apply_prompt_template_TEST, remove_columns=list(testset.features))\n",
    "\n",
    "train_dataset = data.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(data.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "val_dataset = val.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(val.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "test_dataset = test.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(test.features), \n",
    ").map(Concatenator(), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test the model before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "---\n",
      "Summary:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B:\n"
     ]
    }
   ],
   "source": [
    "#Edit eval_prompt to match your data\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enables Parameter Efficient Finetuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n"
     ]
    }
   ],
   "source": [
    "#reduces the parameters needed to train\n",
    "model.train()\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias= \"none\",\n",
    "        target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    kwargs = {\n",
    "        'use_peft': True, \n",
    "        'peft_method': 'lora', \n",
    "        'quantization': True, \n",
    "        'use_fp16': True, \n",
    "        'model_name': os.path.join('/scratch', username, 'models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9'), \n",
    "        'output_dir': os.path.join('/scratch', username)\n",
    "    }\n",
    "    \n",
    "    update_config((train_config, fsdp_config), **kwargs)\n",
    "    \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    #peft_config = generate_peft_config(train_config, kwargs)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "#set up the configurations for training\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1, #2\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkwamea\u001b[0m (\u001b[33mfinetuningnotebook\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kwamea/finetuning_llama2/llama-recipes/wandb/run-20231201_130744-qo7055tc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finetuningnotebook/tmp1/runs/qo7055tc' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/finetuningnotebook/tmp1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finetuningnotebook/tmp1' target=\"_blank\">https://wandb.ai/finetuningnotebook/tmp1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finetuningnotebook/tmp1/runs/qo7055tc' target=\"_blank\">https://wandb.ai/finetuningnotebook/tmp1/runs/qo7055tc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/finetuningnotebook/tmp1/runs/qo7055tc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x154fb468e6e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a weights and biases account and input the api key in below\n",
    "#change the names of the project\n",
    "wandb.init(project=\"tmp1\", name=\"testMistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defines training arguments and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/378 00:12 < 1:19:21, 0.08 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotted train loss\n",
    "torch.cuda.empty_cache()\n",
    "# Define training args\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True, \n",
    "    #logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    eval_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    auto_find_batch_size = True, \n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'},\n",
    "    remove_unused_columns=False,\n",
    "    do_eval=True,\n",
    "    save_steps=10,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,  # Set this to False to avoid conflicts with W&B logging\n",
    "    logging_dir=wandb.run.dir,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "\n",
    "        callbacks=[profiler_callback, WandbCallback()] if enable_profiler else [WandbCallback()],\n",
    "    )\n",
    "    \n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the model to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "#Saves model configurations\n",
    "model.config.to_json_file(os.path.join(output_dir, \"config.json\"))\n",
    "\n",
    "#saves PEFT config\n",
    "peft_config = model.peft_config\n",
    "json_file_path = os.path.join(output_dir, \"peft_config.json\")\n",
    "\n",
    "# Custom serialization function for LoraConfig objects\n",
    "def lora_config_serializer(obj):\n",
    "    if isinstance(obj, LoraConfig):\n",
    "        # Return a dictionary representation of the LoraConfig object\n",
    "        return obj.__dict__\n",
    "    raise TypeError(\"Type not serializable\")\n",
    "\n",
    "# Write the dictionary to the JSON file using the custom serializer\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(peft_config, json_file, default=lora_config_serializer, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Push to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from pathlib import Path\n",
    "#https://huggingface.co/docs/hub/repositories-getting-started\n",
    "api = HfApi()\n",
    "\n",
    "repo_id = \"andrk9/testMistral\"\n",
    "\n",
    "try:\n",
    "    create_repo(repo_id)\n",
    "except RepositoryExistsError:\n",
    "    # If the repository already exists, catch the exception and update the repository\n",
    "    print(f\"Repository '{repo_id}' already exists. Updating...\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "# Usage example after pushing the model to the hub\n",
    "'''\n",
    "repo_name = \"your-username/your-repo-name\"\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(repo_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test model on the same input as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests the model on the sample input from before\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from huggingface\n",
    "repo_id = \"andrk9/testMistral\"\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        model_id=repo_id,\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        token=huggingface_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = PeftModelForCausalLM.from_pretrained(model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(\"\\nGenerated Output: \")\n",
    "    random_idx = random.randint(0, len(test_dataset) - 1)\n",
    "    eval_prompt = test['text'][random_idx]\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS Environment",
   "language": "python",
   "name": "llms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
