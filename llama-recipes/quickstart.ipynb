{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started\n",
    "#### To start this notebook, you must have a huggingface account and request access from Meta to use Llama 2.\n",
    "https://huggingface.co/\n",
    "\n",
    "#### In huggingface, create an access token\n",
    "https://huggingface.co/docs/hub/security-tokens\n",
    "\n",
    "#### Inside your home directory access .apikeys and create a huggingface_api_key.txt and paste you access token inside the file\n",
    "path - /home/{your username}/.apikeys\n",
    "#### Using the link below request access from Meta\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "#### Once you recieve access from Meta inside terminal create a conda environment using\n",
    "conda create --name {environment_name} python=3.10\n",
    "\n",
    "#### Then Install ipykernel using\n",
    "conda install ipykernel\n",
    "\n",
    "#### To allow your environment to be used in the notebook run the following line and select your environment on the top right besides the debugging symbol\n",
    "python -m ipykernel install --user --name={environment_name}\n",
    "\n",
    "#### Go back to terminal and install all the packages with\n",
    "pip install -r packages.txt\n",
    "\n",
    "#### Edit the data set, test set, and validation set under Load Datasets with the path and you are good to go!\n",
    "##### All imported data must be a csv. Csv must have at least 2 columns for output and test\n",
    "##### Import the data inside the llama-recipes folder\n",
    "\n",
    "#### If you are just loading a model run the first 4 cells then skip to the loading section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Huggingface API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/kwamea/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory and load Huggingface api key\n",
    "import os\n",
    "\n",
    "username = os.getenv('USER')\n",
    "directory_path = os.path.join('/scratch', username)\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "\n",
    "# Set Huggingface cache directory to be on scratch drive\n",
    "if os.path.exists(directory_path):\n",
    "    hf_cache_dir = os.path.join(directory_path, 'hf_cache')\n",
    "    if not os.path.exists(hf_cache_dir):\n",
    "        os.mkdir(hf_cache_dir)\n",
    "    print(f\"Okay, using {hf_cache_dir} for huggingface cache. Models will be stored there.\")\n",
    "    assert os.path.exists(hf_cache_dir)\n",
    "    os.environ['TRANSFORMERS_CACHE'] = f'/scratch/{username}/hf_cache/'\n",
    "else:\n",
    "    error_message = f\"Are you sure you entered your username correctly? I couldn't find a directory {directory_path}.\"\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load Huggingface api key\n",
    "api_key_loc = os.path.join('/home', username, '.apikeys', 'huggingface_api_key.txt')\n",
    "\n",
    "if os.path.exists(api_key_loc):\n",
    "    print('Huggingface API key loaded.')\n",
    "    with open(api_key_loc, 'r') as api_key_file:\n",
    "        huggingface_api_key = api_key_file.read().strip()  # Read and store the contents\n",
    "else:\n",
    "    error_message = f'Huggingface API key not found. You need to get an HF API key from the HF website and store it at {api_key_loc}.\\n' \\\n",
    "                    'The API key will let you download models from Huggingface.'\n",
    "    raise FileNotFoundError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, DataCollatorWithPadding, default_data_collator, Trainer, TrainingArguments\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from configs import fsdp_config, train_config\n",
    "from peft import get_peft_model, prepare_model_for_int8_training, PeftModelForCausalLM, LoraConfig, TaskType, prepare_model_for_int8_training, PeftModel\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "from utils.train_utils import (\n",
    "    train,\n",
    "    freeze_transformer_layers,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    clear_gpu_cache,\n",
    "    print_model_size,\n",
    "    #get_policies\n",
    ")\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "from configs.datasets import samsum_dataset, alpaca_dataset, grammar_dataset\n",
    "from ft_datasets.utils import Concatenator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Llama 2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02aa9b4eb835489c8ca727cb2a28e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        token=huggingface_api_key,\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        token=huggingface_api_key\n",
    ")\n",
    "#the code will output \"Error displaying widget: model not found\" it is not an error just the code failing to create a loading bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-04ed86c0369d6106/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-98e7740e620c6c6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (/home/kwamea/.cache/huggingface/datasets/csv/default-ed9c43e8e741b026/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached shuffled indices for dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-04ed86c0369d6106/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cf14eefb8d952c92.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-98e7740e620c6c6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-801b7b4a51fce617.arrow\n",
      "Loading cached shuffled indices for dataset at /home/kwamea/.cache/huggingface/datasets/csv/default-ed9c43e8e741b026/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9de7c49ea3eb65bb.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb65300ee0746198e48b241b78ad962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf8015a14aa4ba4940d4b98f3f8611d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff02ca18ef542dc94df535a7c0f01df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8721263b7244190b7b0fae05836da7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b997eb3ac4e029e0aefa447e93b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fccc340cd7145be8937cab7a93297ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6bdcb9547d473580618e478b31d8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3083b8564314886a665f47bc8f93469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m sample: tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     42\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mfeatures), \n\u001b[1;32m     44\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m valset \u001b[38;5;241m=\u001b[39m valset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m sample: tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     47\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(valset\u001b[38;5;241m.\u001b[39mfeatures), \n\u001b[1;32m     49\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 50\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mtestset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     58\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m valset\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3067\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3068\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3446\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3447\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3449\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3458\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3329\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3330\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3332\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3333\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3334\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m sample: tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     42\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mfeatures), \n\u001b[1;32m     44\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m valset \u001b[38;5;241m=\u001b[39m valset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m sample: tokenizer(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     47\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(valset\u001b[38;5;241m.\u001b[39mfeatures), \n\u001b[1;32m     49\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m testset \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m sample: tokenizer(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[1;32m     52\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(testset\u001b[38;5;241m.\u001b[39mfeatures), \n\u001b[1;32m     54\u001b[0m )\u001b[38;5;241m.\u001b[39mmap(Concatenator(), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     58\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m valset\n",
      "File \u001b[0;32m~/.conda/envs/llms_env/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "#add testset and rename current test set to validation set \n",
    "\n",
    "#edit file path to your unique dataset\n",
    "dataset = load_dataset('csv', data_files='samsum-data/samsum-train.csv',split = 'train')\n",
    "valset = load_dataset('csv', data_files='samsum-data/samsum-validation.csv',split = 'train')\n",
    "testset = load_dataset('csv', data_files='samsum-data/samsum-test.csv',split = 'train')\n",
    "\n",
    "\n",
    "#Sampling only for testing\n",
    "sample_fraction = 0.10\n",
    "\n",
    "num_samples = int(len(dataset) * sample_fraction)\n",
    "num_s = int(len(valset) * sample_fraction)\n",
    "num_sa = int(len(testset) * sample_fraction)\n",
    "\n",
    "# Sample 1/10 of the data randomly\n",
    "dataset = dataset.shuffle(seed=42).select(list(range(num_samples)))\n",
    "valset = valset.shuffle(seed=42).select(list(range(num_s)))\n",
    "testset = testset.shuffle(seed=42).select(list(range(num_sa)))\n",
    "\n",
    "#Edit the prompt to tell the model what to do\n",
    "prompt = (\n",
    "    f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    ")\n",
    "\n",
    "#edit the variables in prompt.format to match your data: essentially what you what the model to read\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            dialog = sample[\"dialogue\"],\n",
    "            summary = sample[\"summary\"],\n",
    "            eos_token=tokenizer.eos_token,\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "valset = valset.map(apply_prompt_template, remove_columns=list(valset.features))\n",
    "testSet = testset.map(apply_prompt_template, remove_columns=list(testset.features))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "valset = valset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(valset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "testset = testset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(testset.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "\n",
    "\n",
    "train_dataset = dataset\n",
    "val_dataset = valset\n",
    "test_dataset = testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the model before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "---\n",
      "Summary:\n",
      "The speaker is asking the other person to go to the animal shelter with him to get a puppy for his son. They discuss what kind of dog they should get and how they should name it.\n",
      "---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Edit eval_prompt to match your data\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Enables Parameter Efficient Finetuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#reduces the parameters needed to train\n",
    "model.train()\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias= \"none\",\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "    \n",
    "    kwargs = {\n",
    "        'use_peft': True, \n",
    "        'peft_method': 'lora', \n",
    "        'quantization': True, \n",
    "        'use_fp16': True, \n",
    "        'model_name': os.path.join('/scratch', username, 'models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9'), \n",
    "        'output_dir': os.path.join('/scratch', username)\n",
    "    }\n",
    "    \n",
    "    update_config((train_config, fsdp_config), **kwargs)\n",
    "    \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    peft_config = generate_peft_config(train_config, kwargs)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "#set up the configurations for training\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defines training arguments and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True, \n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=5,  # 10\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    auto_find_batch_size = True, \n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'},\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=1,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "    \n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save the model to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "#Saves model configurations\n",
    "model.config.to_json_file(os.path.join(output_dir, \"config.json\"))\n",
    "\n",
    "#saves PEFT config\n",
    "peft_config = model.peft_config\n",
    "json_file_path = os.path.join(output_dir, \"peft_config.json\")\n",
    "\n",
    "# Custom serialization function for LoraConfig objects\n",
    "def lora_config_serializer(obj):\n",
    "    if isinstance(obj, LoraConfig):\n",
    "        # Return a dictionary representation of the LoraConfig object\n",
    "        return obj.__dict__\n",
    "    raise TypeError(\"Type not serializable\")\n",
    "\n",
    "# Write the dictionary to the JSON file using the custom serializer\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(peft_config, json_file, default=lora_config_serializer, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test model on the same input as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests the model on the sample input from before\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\neval_prompt = \"\"\"\\nSummarize this dialog:\\nA: Hi Tom, are you busy tomorrow’s afternoon?\\nB: I’m pretty sure I am. What’s up?\\nA: Can you go with me to the animal shelter?.\\nB: What do you want to do?\\nA: I want to get a puppy for my son.\\nB: That will make him so happy.\\nA: Yeah, we’ve discussed it many times. I think he’s ready now.\\nB: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \\nA: I\\'ll get him one of those little dogs.\\nB: One that won\\'t grow up too big;-)\\nA: And eat too much;-))\\nB: Do you know which one he would like?\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\nB: I bet you had to drag him away.\\nA: He wanted to take it home right away ;-).\\nB: I wonder what he\\'ll name it.\\nA: He said he’d name it after his dead hamster – Lemmy  - he\\'s  a great Motorhead fan :-)))\\n---\\nSummary:\\n\"\"\"\\n\\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and test- get metrics and automatically show random 5 model input and output\n",
    "#get this cell running on cold\n",
    "model = PeftModelForCausalLM.from_pretrained(model, output_dir)\n",
    "#import nltk\n",
    "#from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Assuming references is a list of reference texts and hypotheses is a list of generated texts\n",
    "'''\n",
    "def compute_bleu(references, hypotheses):\n",
    "    # Compute BLEU score using nltk's corpus_bleu function\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "'''\n",
    "\n",
    "# Define data collator for evaluation\n",
    "eval_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create Trainer instance for evaluation\n",
    "eval_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_eval_batch_size=8,  # Adjust batch size based on your system's memory\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_training_args,\n",
    "    data_collator=eval_data_collator,\n",
    ")\n",
    "\n",
    "# Evaluate the model on test_dataset\n",
    "evaluation_results = eval_trainer.predict(test_dataset)\n",
    "'''\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Summarize this dialog:\\nClaire: <file_photo>\\r\\nKim: Looks delicious...\\r\\nLinda: No way... Look what I'm cooking right now:\\r\\nLinda: <file_photo>\\r\\nClaire: hahahaha \\r\\nKim: Curry dream team\\r\\nClaire: Enjoy your dinner :*\\n---\\nSummary:\\nBoth Claire and Linda are making curry for dinner. </s>\"}\n"
     ]
    }
   ],
   "source": [
    "print(testSet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " Summarize this dialog:\n",
      "Malik: have you heard of that paleo diet?\n",
      "Malik: i need to lose some weight and i really want to try it\n",
      "Samantha: i've heard of it but i've also heard about the keto diet\n",
      "Samantha: AAAAANNNDDDD... i also need to lose weight lol\n",
      "Malik: what are you talking about?!? lol\n",
      "Malik: you're so skinny\n",
      "Samantha: whatever :-)\n",
      "Malik: should we try one of those together?\n",
      "Malik: it's always easier when someone's doing it with you\n",
      "Samantha: YES!!!!\n",
      "Malik: we can also go for runs together like we used to :-D\n",
      "Samantha: let's do it!! i'm so pumped!\n",
      "Malik: so paleo or keto?\n",
      "Samantha: what's the difference?\n",
      "Malik: i think they're practically the same, but you can't have dairy on paleo\n",
      "Samantha: can you have dairy on keto?\n",
      "Malik: i think you can, i'm no sure though\n",
      "Samantha: ok let me go online and read more about this\n",
      "Samantha: and i'll text you back later with more info\n",
      "Malik: ok\n",
      "Malik: are you excited??\n",
      "Samantha: i really am!!!!!!!!! :-D\n",
      "---\n",
      "Summary:\n",
      "Malik and Samanta want to lose weight. They will try to keep a diet, keto or paleo, and go for runs together.</s>\n",
      "Generated Output: \n",
      "\n",
      "Summarize this dialog:\n",
      "Malik: have you heard of that paleo diet?\n",
      "Malik: i need to lose some weight and i really want to try it\n",
      "Samantha: i've heard of it but i've also heard about the keto diet\n",
      "Samantha: AAAAANNNDDDD... i also need to lose weight lol\n",
      "Malik: what are you talking about?!? lol\n",
      "Malik: you're so skinny\n",
      "Samantha: whatever :-)\n",
      "Malik: should we try one of those together?\n",
      "Malik: it's always easier when someone's doing it with you\n",
      "Samantha: YES!!!!\n",
      "Malik: we can also go for runs together like we used to :-D\n",
      "Samantha: let's do it!! i'm so pumped!\n",
      "Malik: so paleo or keto?\n",
      "Samantha: what's the difference?\n",
      "Malik: i think they're practically the same, but you can't have dairy on paleo\n",
      "Samantha: can you have dairy on keto?\n",
      "Malik: i think you can, i'm no sure though\n",
      "Samantha: ok let me go online and read more about this\n",
      "Samantha: and i'll text you back later with more info\n",
      "Malik: ok\n",
      "Malik: are you excited??\n",
      "Samantha: i really am!!!!!!!!! :-D\n",
      "---\n",
      "Summary:\n",
      "Malik and Samanta want to lose weight. They will try to keep a diet, keto or paleo, and go for runs together. #ifndef __PACKET_H__\n",
      "#define __PACKET_H__\n",
      "\n",
      "#include \"common.h\"\n",
      "\n",
      "#ifdef __cplusplus\n",
      "extern \"C\" {\n",
      "#endif\n",
      "\n",
      "#define PACKET_SIZE (1024 * 1024)\n",
      "\n",
      "struct packet {\n",
      "    uint32_t id;\n",
      "    uint32_t seq;\n",
      "    uint32_\n",
      "Input: \n",
      " Summarize this dialog:\n",
      "Matt: have you heard that Bon Jovi are coming to Poland?\n",
      "Phil: No way!\n",
      "Phil: where, when?\n",
      "Matt: Warsaw, next July\n",
      "Tony: would you like to go?\n",
      "Matt: Sure!\n",
      "Phil: how much are the tickets? Very expensive?\n",
      "Phil: they're huge, I guess tix can cost a bomb\n",
      "Matt: I read the cheapest ones start from pln 250\n",
      "Phil: not too bad, for Bon Jovi\n",
      "Matt: Yep!\n",
      "Tony: maybe I will go too\n",
      "Tony: Would you like to get the tix soon?\n",
      "Matt: Yes\n",
      "Matt: The sooner, the better\n",
      "Tony: Right, they may be sold out pretty soon\n",
      "Matt: How about you, Phil?\n",
      "Matt: would you like to go too?\n",
      "Phil: Will think about it\n",
      "Phil: I liked them a lot when I was a kid\n",
      "Phil: It would be great to see them live\n",
      "Matt: I guess so!\n",
      "Matt: Think about it and let me know soon, ok?\n",
      "Phil: Sure\n",
      "---\n",
      "Summary:\n",
      "Matt and Tony want to go to the concert of Bon Jovi next July in Poland and are planning to buy the tickets that cost 250 PLN before they’re sold out. Phil will think about it and let them know.</s>\n",
      "Generated Output: \n",
      "\n",
      "Summarize this dialog:\n",
      "Matt: have you heard that Bon Jovi are coming to Poland?\n",
      "Phil: No way!\n",
      "Phil: where, when?\n",
      "Matt: Warsaw, next July\n",
      "Tony: would you like to go?\n",
      "Matt: Sure!\n",
      "Phil: how much are the tickets? Very expensive?\n",
      "Phil: they're huge, I guess tix can cost a bomb\n",
      "Matt: I read the cheapest ones start from pln 250\n",
      "Phil: not too bad, for Bon Jovi\n",
      "Matt: Yep!\n",
      "Tony: maybe I will go too\n",
      "Tony: Would you like to get the tix soon?\n",
      "Matt: Yes\n",
      "Matt: The sooner, the better\n",
      "Tony: Right, they may be sold out pretty soon\n",
      "Matt: How about you, Phil?\n",
      "Matt: would you like to go too?\n",
      "Phil: Will think about it\n",
      "Phil: I liked them a lot when I was a kid\n",
      "Phil: It would be great to see them live\n",
      "Matt: I guess so!\n",
      "Matt: Think about it and let me know soon, ok?\n",
      "Phil: Sure\n",
      "---\n",
      "Summary:\n",
      "Matt and Tony want to go to the concert of Bon Jovi next July in Poland and are planning to buy the tickets that cost 250 PLN before they’re sold out. Phil will think about it and let them know. #include \"stdafx.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "#include \"XnLog.h\"\n",
      "Input: \n",
      " Summarize this dialog:\n",
      "Malik: have you heard of that paleo diet?\n",
      "Malik: i need to lose some weight and i really want to try it\n",
      "Samantha: i've heard of it but i've also heard about the keto diet\n",
      "Samantha: AAAAANNNDDDD... i also need to lose weight lol\n",
      "Malik: what are you talking about?!? lol\n",
      "Malik: you're so skinny\n",
      "Samantha: whatever :-)\n",
      "Malik: should we try one of those together?\n",
      "Malik: it's always easier when someone's doing it with you\n",
      "Samantha: YES!!!!\n",
      "Malik: we can also go for runs together like we used to :-D\n",
      "Samantha: let's do it!! i'm so pumped!\n",
      "Malik: so paleo or keto?\n",
      "Samantha: what's the difference?\n",
      "Malik: i think they're practically the same, but you can't have dairy on paleo\n",
      "Samantha: can you have dairy on keto?\n",
      "Malik: i think you can, i'm no sure though\n",
      "Samantha: ok let me go online and read more about this\n",
      "Samantha: and i'll text you back later with more info\n",
      "Malik: ok\n",
      "Malik: are you excited??\n",
      "Samantha: i really am!!!!!!!!! :-D\n",
      "---\n",
      "Summary:\n",
      "Malik and Samanta want to lose weight. They will try to keep a diet, keto or paleo, and go for runs together.</s>\n",
      "Generated Output: \n",
      "\n",
      "Summarize this dialog:\n",
      "Malik: have you heard of that paleo diet?\n",
      "Malik: i need to lose some weight and i really want to try it\n",
      "Samantha: i've heard of it but i've also heard about the keto diet\n",
      "Samantha: AAAAANNNDDDD... i also need to lose weight lol\n",
      "Malik: what are you talking about?!? lol\n",
      "Malik: you're so skinny\n",
      "Samantha: whatever :-)\n",
      "Malik: should we try one of those together?\n",
      "Malik: it's always easier when someone's doing it with you\n",
      "Samantha: YES!!!!\n",
      "Malik: we can also go for runs together like we used to :-D\n",
      "Samantha: let's do it!! i'm so pumped!\n",
      "Malik: so paleo or keto?\n",
      "Samantha: what's the difference?\n",
      "Malik: i think they're practically the same, but you can't have dairy on paleo\n",
      "Samantha: can you have dairy on keto?\n",
      "Malik: i think you can, i'm no sure though\n",
      "Samantha: ok let me go online and read more about this\n",
      "Samantha: and i'll text you back later with more info\n",
      "Malik: ok\n",
      "Malik: are you excited??\n",
      "Samantha: i really am!!!!!!!!! :-D\n",
      "---\n",
      "Summary:\n",
      "Malik and Samanta want to lose weight. They will try to keep a diet, keto or paleo, and go for runs together. #include \"test/cpp/api/test_util.h\"\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"api/test/cpp/api_test_util.h\"\n",
      "#include \"api/test/cpp/fake_rtp_receiver.h\"\n",
      "#include \"rtc_base/checks.h\"\n",
      "#include \"rtc_base/numerics/safe_conversions.\n",
      "Input: \n",
      " Summarize this dialog:\n",
      "Nathalie: have you thought about the holiday?\n",
      "Pauline: me & tony are into greece really\n",
      "Jacob: anywhere warm and sunny. greece cool\n",
      "Anthony: greece is warm sunny and cheapish\n",
      "Nathalie: i guess cob we ok w that\n",
      "Jacob: sure thing\n",
      "Pauline: so august as we said?\n",
      "Jacob: thats the thing. we need to be back by aug 10\n",
      "Anthony: what?? why??\n",
      "Nathalie: sis wedding\n",
      "Pauline: your lil sis getting married?!? lol\n",
      "Jacob: she's not little. seen her tony?\n",
      "Anthony: worth a look?\n",
      "Nathalie: shut up assholes. shes my sister for fucks sake\n",
      "Pauline: idiots\n",
      "Jacob: come one just kidding. we love you\n",
      "Anthony: we have no choice XD\n",
      "---\n",
      "Summary:\n",
      "Nathalie, Pauline, Jacob and Anthony are thinking about spending holidays in Greece together in August. Jacob and Nathalie need to be back by August 10 because of Nathalie's younger sister's wedding. </s>\n",
      "Generated Output: \n",
      "\n",
      "Summarize this dialog:\n",
      "Nathalie: have you thought about the holiday?\n",
      "Pauline: me & tony are into greece really\n",
      "Jacob: anywhere warm and sunny. greece cool\n",
      "Anthony: greece is warm sunny and cheapish\n",
      "Nathalie: i guess cob we ok w that\n",
      "Jacob: sure thing\n",
      "Pauline: so august as we said?\n",
      "Jacob: thats the thing. we need to be back by aug 10\n",
      "Anthony: what?? why??\n",
      "Nathalie: sis wedding\n",
      "Pauline: your lil sis getting married?!? lol\n",
      "Jacob: she's not little. seen her tony?\n",
      "Anthony: worth a look?\n",
      "Nathalie: shut up assholes. shes my sister for fucks sake\n",
      "Pauline: idiots\n",
      "Jacob: come one just kidding. we love you\n",
      "Anthony: we have no choice XD\n",
      "---\n",
      "Summary:\n",
      "Nathalie, Pauline, Jacob and Anthony are thinking about spending holidays in Greece together in August. Jacob and Nathalie need to be back by August 10 because of Nathalie's younger sister's wedding.  import 'dart:io';\n",
      "\n",
      "import 'package:flutter/material.dart';\n",
      "import 'package:flutter_bloc/flutter_bloc.dart';\n",
      "import 'package:flutter_stars/app/app.dart';\n",
      "import 'package:flutter_stars/app/app_state.dart';\n",
      "import 'package:flutter_stars/app/components/app_bar.dart';\n",
      "import 'package:flutter_stars/\n",
      "Input: \n",
      " Summarize this dialog:\n",
      "Matt: have you heard that Bon Jovi are coming to Poland?\n",
      "Phil: No way!\n",
      "Phil: where, when?\n",
      "Matt: Warsaw, next July\n",
      "Tony: would you like to go?\n",
      "Matt: Sure!\n",
      "Phil: how much are the tickets? Very expensive?\n",
      "Phil: they're huge, I guess tix can cost a bomb\n",
      "Matt: I read the cheapest ones start from pln 250\n",
      "Phil: not too bad, for Bon Jovi\n",
      "Matt: Yep!\n",
      "Tony: maybe I will go too\n",
      "Tony: Would you like to get the tix soon?\n",
      "Matt: Yes\n",
      "Matt: The sooner, the better\n",
      "Tony: Right, they may be sold out pretty soon\n",
      "Matt: How about you, Phil?\n",
      "Matt: would you like to go too?\n",
      "Phil: Will think about it\n",
      "Phil: I liked them a lot when I was a kid\n",
      "Phil: It would be great to see them live\n",
      "Matt: I guess so!\n",
      "Matt: Think about it and let me know soon, ok?\n",
      "Phil: Sure\n",
      "---\n",
      "Summary:\n",
      "Matt and Tony want to go to the concert of Bon Jovi next July in Poland and are planning to buy the tickets that cost 250 PLN before they’re sold out. Phil will think about it and let them know.</s>\n",
      "Generated Output: \n",
      "\n",
      "Summarize this dialog:\n",
      "Matt: have you heard that Bon Jovi are coming to Poland?\n",
      "Phil: No way!\n",
      "Phil: where, when?\n",
      "Matt: Warsaw, next July\n",
      "Tony: would you like to go?\n",
      "Matt: Sure!\n",
      "Phil: how much are the tickets? Very expensive?\n",
      "Phil: they're huge, I guess tix can cost a bomb\n",
      "Matt: I read the cheapest ones start from pln 250\n",
      "Phil: not too bad, for Bon Jovi\n",
      "Matt: Yep!\n",
      "Tony: maybe I will go too\n",
      "Tony: Would you like to get the tix soon?\n",
      "Matt: Yes\n",
      "Matt: The sooner, the better\n",
      "Tony: Right, they may be sold out pretty soon\n",
      "Matt: How about you, Phil?\n",
      "Matt: would you like to go too?\n",
      "Phil: Will think about it\n",
      "Phil: I liked them a lot when I was a kid\n",
      "Phil: It would be great to see them live\n",
      "Matt: I guess so!\n",
      "Matt: Think about it and let me know soon, ok?\n",
      "Phil: Sure\n",
      "---\n",
      "Summary:\n",
      "Matt and Tony want to go to the concert of Bon Jovi next July in Poland and are planning to buy the tickets that cost 250 PLN before they’re sold out. Phil will think about it and let them know. import { Component, OnInit, ViewChild, ViewEncapsulation, ChangeDetectorRef } from '@angular/core';\n",
      "import { Router, ActivatedRoute } from '@angular/router';\n",
      "import { FormBuilder, FormGroup, Validators } from '@angular/forms';\n",
      "import { MatSnackBar } from '@angular/material/snack-bar';\n",
      "import { UserService } from '../shared/user.service';\n",
      "import { User\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    random_idx = random.randint(0, len(test_dataset) - 1)\n",
    "    eval_prompt = testSet[random_idx]['text']\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(\"Input: \\n\", eval_prompt)\n",
    "    model.eval()\n",
    "    print(\"Generated Output: \\n\")\n",
    "    with torch.no_grad():\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS Environment",
   "language": "python",
   "name": "llms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
