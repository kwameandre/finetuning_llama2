{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9ed5a6-14c3-422a-8521-203e3d0eee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/kwamea/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory and load Huggingface api key\n",
    "import os\n",
    "\n",
    "username = os.getenv('USER')\n",
    "directory_path = os.path.join('/scratch', username)\n",
    "output_dir = os.path.join('/scratch', username ,'llama-output')\n",
    "\n",
    "# Set Huggingface cache directory to be on scratch drive\n",
    "if os.path.exists(directory_path):\n",
    "    hf_cache_dir = os.path.join(directory_path, 'hf_cache')\n",
    "    if not os.path.exists(hf_cache_dir):\n",
    "        os.mkdir(hf_cache_dir)\n",
    "    print(f\"Okay, using {hf_cache_dir} for huggingface cache. Models will be stored there.\")\n",
    "    assert os.path.exists(hf_cache_dir)\n",
    "    os.environ['TRANSFORMERS_CACHE'] = f'/scratch/{username}/hf_cache/'\n",
    "else:\n",
    "    error_message = f\"Are you sure you entered your username correctly? I couldn't find a directory {directory_path}.\"\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load Huggingface api key\n",
    "api_key_loc = os.path.join('/home', username, '.apikeys', 'huggingface_api_key.txt')\n",
    "\n",
    "if os.path.exists(api_key_loc):\n",
    "    print('Huggingface API key loaded.')\n",
    "    with open(api_key_loc, 'r') as api_key_file:\n",
    "        huggingface_api_key = api_key_file.read().strip()  # Read and store the contents\n",
    "else:\n",
    "    error_message = f'Huggingface API key not found. You need to get an HF API key from the HF website and store it at {api_key_loc}.\\n' \\\n",
    "                    'The API key will let you download models from Huggingface.'\n",
    "    raise FileNotFoundError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8382b5-0f93-48a6-80e3-91755d451490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwamea/.conda/envs/llms_env/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/kwamea/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, DataCollatorWithPadding, default_data_collator, Trainer, TrainingArguments\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from configs import fsdp_config, train_config\n",
    "from peft import get_peft_model, prepare_model_for_int8_training, PeftModelForCausalLM, LoraConfig, TaskType, prepare_model_for_int8_training, PeftModel\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "from utils.train_utils import (\n",
    "    train,\n",
    "    freeze_transformer_layers,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    clear_gpu_cache,\n",
    "    print_model_size,\n",
    ")\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from datasets import Dataset,load_dataset, ClassLabel, Features, Array2D\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "from configs.datasets import samsum_dataset, alpaca_dataset, grammar_dataset\n",
    "from ft_datasets.utils import Concatenator\n",
    "import huggingface_hub\n",
    "from huggingface_hub import notebook_login, Repository, HfApi, create_repo, delete_repo\n",
    "huggingface_hub.login(token = huggingface_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6235550-52b9-4994-bf5d-e186aac4d01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-2-70b-hf\", #mistralai/Mistral-7B-v0.1 mistralai/Mixtral-8x7B-v0.1 meta-llama/Llama-2-70b-hf\n",
    "        resume_download=True,\n",
    "        cache_dir=os.path.join('/scratch', username),\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        token=huggingface_api_key,\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": \"<PAD>\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de77ebc-45db-45c8-aaed-0ecd9829e782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac784e5af9e408d92f9ab4cff14a61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "#loads plain model\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "plain_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-70b-hf\",\n",
    "    resume_download=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        float16_bits=8,  # Adjust as needed\n",
    "        float32_bits=32,  # Adjust as needed\n",
    "    ),\n",
    "    device_map=\"auto\" if train_config.quantization else None,\n",
    "    cache_dir=os.path.join('/scratch', username),\n",
    "    trust_remote_code=True,\n",
    "    token=huggingface_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c03a2-a037-4e88-aba0-25e78c438201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads datasets\n",
    "\n",
    "#add testset and rename current test set to validation set \n",
    "\n",
    "#edit file path to your unique dataset\n",
    "\n",
    "user_input = input(\"Do you want to input your own test splits? (yes/no): \")\n",
    "\n",
    "if user_input.lower() == \"yes\":\n",
    "    dataset = load_dataset('csv', data_files='samsum-data/samsum-train.csv',split = 'train')\n",
    "    valset = load_dataset('csv', data_files='samsum-data/samsum-validation.csv',split = 'train')\n",
    "    testset = load_dataset('csv', data_files='samsum-data/samsum-test.csv',split = 'train')\n",
    "\n",
    "#creates train, test, and validation splits\n",
    "else:\n",
    "    # Load the dataset from a CSV file\n",
    "    full_dataset = load_dataset('csv', data_files='combined_info.csv')\n",
    "\n",
    "    # Get the number of examples in the dataset\n",
    "    num_examples = len(full_dataset[\"train\"])\n",
    "\n",
    "    # Define the split ratios\n",
    "    train_ratio = 0.8\n",
    "    validation_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    # Calculate the number of examples for each split\n",
    "    num_train_examples = int(num_examples * train_ratio)\n",
    "    num_validation_examples = int(num_examples * validation_ratio)\n",
    "    num_test_examples = int(num_examples * test_ratio)\n",
    "\n",
    "    # Split the dataset\n",
    "    splits = full_dataset[\"train\"].train_test_split(\n",
    "        test_size=num_test_examples,\n",
    "        train_size=num_train_examples,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Assign the splits to variables\n",
    "    dataset = splits[\"train\"]\n",
    "    valset = splits[\"test\"]\n",
    "\n",
    "    # If you want a separate test split, you can use the test split from the original split\n",
    "    testset = full_dataset[\"train\"].train_test_split(\n",
    "        test_size=num_test_examples,\n",
    "        train_size=num_train_examples\n",
    "    )[\"test\"]\n",
    "\n",
    "# Now you can use train_dataset, validation_dataset, and test_dataset for training, validation, and testing\n",
    "\n",
    "#####TODO change the prompt to ask based on the given cat what theme best fits\n",
    "#Edit the prompt to tell the model what to do including the variables from prompt.format\n",
    "prompt = (\n",
    "    f\"Generate labels that best fit the following text with respect topic:\\n{{text}}\\n---\\nLabel:{{label}}\\nLabels:\\n\"\n",
    ")\n",
    "\n",
    "#prompt for testing\n",
    "test_prompt = (\n",
    "    f\"Tell me which labels best fit the following text:\\n{{text}}\\n---\\nLabel:\\n\"\n",
    ")\n",
    "\n",
    "#edit the variables in prompt.format to match your data: essentially what you what the model to read\n",
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            text = sample[\"text\"],\n",
    "            label = sample[\"label\"],\n",
    "        )\n",
    "    }\n",
    "\n",
    "#Only include what you want the model to see during testing\n",
    "def apply_prompt_template_TEST(sample):\n",
    "    return {\n",
    "        \"text\": test_prompt.format(\n",
    "            text = sample[\"text\"],\n",
    "        )\n",
    "    }\n",
    "\n",
    "data = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "val = valset.map(apply_prompt_template, remove_columns=list(valset.features))\n",
    "test = testset.map(apply_prompt_template_TEST, remove_columns=list(testset.features))\n",
    "\n",
    "train_dataset = data.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(data.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "val_dataset = val.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(val.features), \n",
    ").map(Concatenator(), batched=True)\n",
    "test_dataset = test.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]),\n",
    "    batched=True,\n",
    "    remove_columns=list(test.features), \n",
    ").map(Concatenator(), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c69901-2c79-4ab0-9f81-9fb97cbccae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize this dialog:\n",
      "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
      "B: I’m pretty sure I am. What’s up?\n",
      "A: Can you go with me to the animal shelter?.\n",
      "B: What do you want to do?\n",
      "A: I want to get a puppy for my son.\n",
      "B: That will make him so happy.\n",
      "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
      "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
      "A: I'll get him one of those little dogs.\n",
      "B: One that won't grow up too big;-)\n",
      "A: And eat too much;-))\n",
      "B: Do you know which one he would like?\n",
      "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
      "B: I bet you had to drag him away.\n",
      "A: He wanted to take it home right away ;-).\n",
      "B: I wonder what he'll name it.\n",
      "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
      "---\n",
      "Summary:\n",
      "A: A wants to get a puppy for her son.\n",
      "B: B suggests to go to the animal shelter.\n",
      "A: A asks B to go with her to the animal shelter.\n",
      "B: B asks what A wants to do.\n",
      "A: A wants to get a puppy for her son.\n",
      "B: B says that it will make him so happy.\n",
      "A: A says that they have discussed it many times and that she thinks that he is ready now.\n"
     ]
    }
   ],
   "source": [
    "#prompt for plain model\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "plain_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acfc354-0903-435c-8a90-9b5213f8dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads finetuned model\n",
    "finetuned_model = PeftModelForCausalLM.from_pretrained(plain_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416295d-8c73-4b9b-8232-7bb5bae3f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt for finetuned model\n",
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
    "B: I’m pretty sure I am. What’s up?\n",
    "A: Can you go with me to the animal shelter?.\n",
    "B: What do you want to do?\n",
    "A: I want to get a puppy for my son.\n",
    "B: That will make him so happy.\n",
    "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
    "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-) \n",
    "A: I'll get him one of those little dogs.\n",
    "B: One that won't grow up too big;-)\n",
    "A: And eat too much;-))\n",
    "B: Do you know which one he would like?\n",
    "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
    "B: I bet you had to drag him away.\n",
    "A: He wanted to take it home right away ;-).\n",
    "B: I wonder what he'll name it.\n",
    "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a398f-d273-402c-a02f-023cb221f901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS Environment",
   "language": "python",
   "name": "llms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
